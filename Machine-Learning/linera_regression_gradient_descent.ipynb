{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(237, 4)\n[[1.         0.88406512]\n [1.         0.50222003]\n [1.         0.7602368 ]\n [1.         0.52146029]\n [1.         0.71879625]\n [1.         0.42673902]\n [1.         0.52540701]\n [1.         0.41391219]\n [1.         0.44055254]\n [1.         0.62259497]\n [1.         0.35668476]\n [1.         0.62802171]\n [1.         0.45387272]\n [1.         0.73408979]\n [1.         0.54859398]\n [1.         0.57030094]\n [1.         0.38332511]\n [1.         0.36803157]\n [1.         0.18500247]\n [1.         0.84065121]\n [1.         0.57128762]\n [1.         0.65416872]\n [1.         0.53478046]\n [1.         0.48840651]\n [1.         1.        ]\n [1.         0.84015787]\n [1.         0.64923532]\n [1.         0.64232856]\n [1.         0.36211149]\n [1.         0.71780957]\n [1.         0.52639369]\n [1.         0.53083374]\n [1.         0.6822891 ]\n [1.         0.71090281]\n [1.         0.70942279]\n [1.         0.53971386]\n [1.         0.39812531]\n [1.         0.50715343]\n [1.         0.30291071]\n [1.         0.38085841]\n [1.         0.61272817]\n [1.         0.38727183]\n [1.         0.78687716]\n [1.         0.53478046]\n [1.         0.56388752]\n [1.         0.64824864]\n [1.         0.78342378]\n [1.         0.21953626]\n [1.         0.45436606]\n [1.         0.45584608]\n [1.         0.57770104]\n [1.         0.52935372]\n [1.         0.76467686]\n [1.         0.6625555 ]\n [1.         0.63739517]\n [1.         0.36408485]\n [1.         0.5772077 ]\n [1.         0.71336951]\n [1.         0.59940799]\n [1.         0.46817958]\n [1.         0.56536754]\n [1.         0.33201776]\n [1.         0.84953133]\n [1.         0.75629008]\n [1.         0.49679329]\n [1.         0.30044401]\n [1.         0.34287124]\n [1.         0.32165762]\n [1.         0.84361125]\n [1.         0.81943759]\n [1.         0.63542181]\n [1.         0.56142082]\n [1.         0.69116922]\n [1.         0.65959546]\n [1.         0.54464726]\n [1.         0.3325111 ]\n [1.         0.41341885]\n [1.         0.31672422]\n [1.         0.59694129]\n [1.         0.550074  ]\n [1.         0.5476073 ]\n [1.         0.56043414]\n [1.         0.26097681]\n [1.         0.4227923 ]\n [1.         0.59842131]\n [1.         0.5574741 ]\n [1.         0.29057721]\n [1.         0.33843118]\n [1.         0.38776517]\n [1.         0.58559447]\n [1.         0.71040947]\n [1.         0.29501727]\n [1.         0.4647262 ]\n [1.         0.58164776]\n [1.         0.48347311]\n [1.         0.52244697]\n [1.         0.37148495]\n [1.         0.37987173]\n [1.         0.46077948]\n [1.         0.37395165]\n [1.         0.38233843]\n [1.         0.54958066]\n [1.         0.57030094]\n [1.         0.46423286]\n [1.         0.44301924]\n [1.         0.45781944]\n [1.         0.64726196]\n [1.         0.3349778 ]\n [1.         0.59003453]\n [1.         0.84361125]\n [1.         0.48100641]\n [1.         0.39664529]\n [1.         0.41983226]\n [1.         0.43117908]\n [1.         0.32708436]\n [1.         0.38431179]\n [1.         0.42871238]\n [1.         0.5821411 ]\n [1.         0.68771584]\n [1.         0.60039467]\n [1.         0.3349778 ]\n [1.         0.73014307]\n [1.         0.87222496]\n [1.         0.44104588]\n [1.         0.65663542]\n [1.         0.52392699]\n [1.         0.33103108]\n [1.         0.19930932]\n [1.         0.6576221 ]\n [1.         0.425259  ]\n [1.         0.46669956]\n [1.         0.40059201]\n [1.         0.65416872]\n [1.         0.4671929 ]\n [1.         0.06758757]\n [1.         0.35323138]\n [1.         0.52836704]\n [1.         0.28712383]\n [1.         0.18944253]\n [1.         0.2224963 ]\n [1.         0.4203256 ]\n [1.         0.39960533]\n [1.         0.22446966]\n [1.         0.35421806]\n [1.         0.58362111]\n [1.         0.58164776]\n [1.         0.33596448]\n [1.         0.26985693]\n [1.         0.36063148]\n [1.         0.18253577]\n [1.         0.34188456]\n [1.         0.29748397]\n [1.         0.47360631]\n [1.         0.3547114 ]\n [1.         0.55895412]\n [1.         0.2150962 ]\n [1.         0.27577701]\n [1.         0.48692649]\n [1.         0.63443513]\n [1.         0.27084361]\n [1.         0.17316231]\n [1.         0.52244697]\n [1.         0.40848545]\n [1.         0.28219043]\n [1.         0.38332511]\n [1.         0.17858905]\n [1.         0.26048347]\n [1.         0.31475086]\n [1.         0.53428712]\n [1.         0.41736556]\n [1.         0.20966946]\n [1.         0.38628515]\n [1.         0.41983226]\n [1.         0.49531327]\n [1.         0.44153922]\n [1.         0.23828318]\n [1.         0.43857918]\n [1.         0.41489887]\n [1.         0.62111495]\n [1.         0.40108535]\n [1.         0.47804637]\n [1.         0.21608288]\n [1.         0.63394179]\n [1.         0.2274297 ]\n [1.         0.37444499]\n [1.         0.4548594 ]\n [1.         0.449926  ]\n [1.         0.17217563]\n [1.         0.3325111 ]\n [1.         0.48495313]\n [1.         0.21953626]\n [1.         0.3127775 ]\n [1.         0.13813518]\n [1.         0.47705969]\n [1.         0.41243217]\n [1.         0.02614702]\n [1.         0.16674889]\n [1.         0.3078441 ]\n [1.         0.38135175]\n [1.         0.28465713]\n [1.         0.31573754]\n [1.         0.25061667]\n [1.         0.27479033]\n [1.         0.55796744]\n [1.         0.17118895]\n [1.         0.47952639]\n [1.         0.33645782]\n [1.         0.62900839]\n [1.         0.29501727]\n [1.         0.        ]\n [1.         0.10705476]\n [1.         0.42427232]\n [1.         0.10804144]\n [1.         0.13270844]\n [1.         0.42723236]\n [1.         0.2150962 ]\n [1.         0.25949679]\n [1.         0.22200296]\n [1.         0.27035027]\n [1.         0.3300444 ]\n [1.         0.32609768]\n [1.         0.07104095]\n [1.         0.50320671]\n [1.         0.37444499]\n [1.         0.4573261 ]\n [1.         0.49136655]\n [1.         0.27824371]\n [1.         0.73211643]\n [1.         0.50074001]\n [1.         0.24568328]\n [1.         0.47607301]\n [1.         0.48544647]\n [1.         0.24370992]\n [1.         0.3325111 ]\n [1.         0.25308337]\n [1.         0.31179082]\n [1.         0.33103108]]\n"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv('headbrain.csv')\n",
    "print(dataset.shape)\n",
    "\n",
    "X = dataset['Head Size(cm^3)'].values\n",
    "X = np.apply_along_axis(lambda c: (c - c.min())/(c.max() - c.min()), 0, X)\n",
    "Y = dataset['Brain Weight(grams)'].values\n",
    "Y = np.apply_along_axis(lambda c: (c - c.min())/(c.max() - c.min()), 0, Y)\n",
    "Y = np.reshape(Y, (len(Y), 1))\n",
    "\n",
    "one = np.ones(len(X))\n",
    "\n",
    "X = np.vstack((one, X)).T\n",
    "print(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_pred, y_act):\n",
    " \n",
    "    n = len(y_act)\n",
    "    rmse = np.sqrt((np.sum((y_act - y_pred) ** 2) / n)) \n",
    "\n",
    "    return rmse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression(X_train, X_test, Y_train, Y_test, itr, l_rate):\n",
    "    \n",
    "    theta = np.zeros((X_train.shape[1], 1))\n",
    "    c = l_rate / len(X_train)\n",
    "    train_rmse = np.empty(itr)\n",
    "    test_rmse = np.empty(itr)\n",
    "    h_train = X_train @ theta\n",
    "\n",
    "\n",
    "    for i in range(itr):\n",
    "        theta = theta - c * (X_train.T @ (h_train - Y_train))\n",
    "        h_train = X_train @ theta\n",
    "        h_test = X_test @ theta\n",
    "\n",
    "        train_rmse[i] = rmse(h_train, Y_train)\n",
    "        test_rmse[i] = rmse(h_test, Y_test)\n",
    "    \n",
    "    print(theta)\n",
    "\n",
    "    return train_rmse, test_rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[[0.36869324]\n [0.26086599]]\n[0.51033728 0.50977611 0.50921568 ... 0.1413195  0.14131774 0.14131598]\n[0.52299572 0.52242774 0.5218605  ... 0.1445495  0.14454787 0.14454624]\n"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 1)\n",
    "itr = 5000\n",
    "l_rate = 0.001\n",
    "train_rmse = np.empty(itr)\n",
    "test_rmse = np.empty(itr)\n",
    "train_rmse, test_rmse = regression(X_train, X_test, Y_train, Y_test, itr, l_rate)\n",
    "print(train_rmse)\n",
    "print(test_rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}